# Introduction

Large language models (LLMs) can be trained on a vast number of high-quality code repositories. These models can then be used for a variety of software engineering tasks including code generation.

The figure below shows an overview of the process:

![LLM](https://media.licdn.com/dms/image/D4D12AQEj4ADRPfqFyw/article-cover_image-shrink_720_1280/0/1701459621193?e=2147483647&v=beta&t=Lb-e_yR3Rb3DTWK5Ob0t5RXp6BoMiVHm8yGKVsLUvUI)

<img src="https://media.licdn.com/dms/image/D4D12AQEj4ADRPfqFyw/article-cover_image-shrink_720_1280/0/1701459621193?e=2147483647&v=beta&t=Lb-e_yR3Rb3DTWK5Ob0t5RXp6BoMiVHm8yGKVsLUvUI" alt="drawing" width="500"/>

Credit: Image by [Shivasish Mahapatra](https://www.linkedin.com/in/shivmahapatra?trk=article-ssr-frontend-pulse_publisher-author-card&originalSubdomain=in).

In the figure above, the model is trained on a large amount high-quality code that has been written by hand. The trained model can then be used to solve software engineering problems. For instance, it can be prompted to generate a custom function in the Python programming language.

A.I. tools can be used for:

- code generation  
- language translation
- debugging assistance
- code navigation  
- writing test code  
- generating documentation  
- parallelizing code
- IDE integration with code completion

## Pros and Cons of A.I. versus conventional coding

When should a researcher try to use one of the A.I. tools as opposed to convential methods?

When should a researcher expect to write their own code?

What are the limitations of LLMs for software engineering?

What does the future look like?

## Responsibility

What if a researcher publishes their results using A.I. generated code that has a bug?

## Security
